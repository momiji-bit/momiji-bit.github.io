---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>
I am currently a **Research Assistant** at the [Computational Light Laboratory](https://complightlab.com), University College London, working under the supervision of [**Assoc. Kaan AkÅŸit**](https://www.kaanaksit.com/). I am also in the process of applying for PhD programs in Computer Vision.

I recently completed my **MSc in Computer Graphics, Vision and Imaging** at [University College London](https://www.ucl.ac.uk/). Under the supervision of [**Assoc. Kaan AkÅŸit**](https://www.kaanaksit.com/), my research centers on text-guided video anomaly detection (VAD) based on Large Vision-Language Models (LVLMs), aiming to enable fine-grained, interpretable, and human-centered video understanding.

I received my **BEng in Computer Science and Technology** from [Hefei University of Technology](https://ci.hfut.edu.cn/), School of Computer Science and Information Engineering (School of Artificial Intelligence), where I was supervised by [**Prof. Dan Guo**](https://faculty.hfut.edu.cn/gd/en/index.htm). During my undergraduate studies, I conducted research on visual perception systems for assisting visually impaired individuals, exploring multimodal sensing, intelligent interaction, and navigation technologies.

My research interests span **computer vision**, **multimodal learning**, and **vision-language understanding**, with an emphasis on perception, reasoning, and generation in visual intelligence. I am particularly interested in building systems that connect human motion, emotion, and cognition through multimodal signals, with additional interests in autonomous driving and environmental perception.

If you are seeking any form of academic collaboration, please feel free to contact me. <a href='https://scholar.google.com/citations?user=fSWwq3AAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>


# ğŸ”¥ News
- *2025.07*:  ğŸ‰ğŸ‰ğŸ‰ Our paper accepted by **ACM MM 2025**.
- *2025.05*:  ğŸ†ğŸ†ğŸ† Our work won the champion of Micro-gesture Classification sub-challenge in **MiGA@IJCAI 2025**.
- *2024.06*:  ğŸ†ğŸ†ğŸ† My undergraduate thesis was recognized as the **Best Thesis Award** at Hefei University of Technology.

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/MABench.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**MA-Bench: Towards Fine-grained Micro-Action Understanding** (*under-review*)

[Preprint] [Paper] [Project]

Kun Li, **Jihao Gu**, Fei Wang, zhiliang wu, Hehe Fan, Dan Guo

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Preprint</div><img src='images/T-VAD.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Text-guided Fine-Grained Video Anomaly Detection** (*under-review*)

[Preprint] [[Paper](https://arxiv.org/abs/2511.00524)] [Project]

**Jihao Gu**, Kun Li, Wang He, AkÅŸit Kaan

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/MMN.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition

[[ACM MM'25](https://acmmm2025.org/)] [[Paper](https://dl.acm.org/doi/abs/10.1145/3746027.3754722)] [[Project](https://github.com/momiji-bit/MMN)] <a href="https://visitor-badge.laobi.icu/badge?page_id=momiji-bit.MMN&left_color=green&right_color=red" target="_blank"><img src="https://visitor-badge.laobi.icu/badge?page_id=momiji-bit.MMN&left_color=green&right_color=red"></a> <a href="https://img.shields.io/github/stars/momiji-bit/MMN?style=flat&color=yellow" target="_blank"><img src="https://img.shields.io/github/stars/momiji-bit/MMN?style=flat&color=yellow"></a>

**Jihao Gu**, Kun Li, Fei Wang, Yanyan Wei, Zhiliang Wu, Hehe Fan, Meng Wang

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2025</div><img src='images/MMG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion

[[IJCAI'25 Workshop](https://2025.ijcai.org/)] [[Paper](https://arxiv.org/abs/2507.08344)] [[Project](https://github.com/momiji-bit/MM-Gesture)] <a href="https://visitor-badge.laobi.icu/badge?page_id=momiji-bit.MM-Gesture&left_color=green&right_color=red" target="_blank"><img src="https://visitor-badge.laobi.icu/badge?page_id=momiji-bit.MM-Gesture&left_color=green&right_color=red"></a> <a href="https://img.shields.io/github/stars/momiji-bit/MM-Gesture?style=flat&color=yellow" target="_blank"><img src="https://img.shields.io/github/stars/momiji-bit/MM-Gesture?style=flat&color=yellow"></a>

**Jihao Gu**, Fei Wang, Kun Li, Yanyan Wei, Zhiliang Wu, Dan Guo

> ğŸ† The Champion of Micro-gesture Classification sub-challenge in [MiGA@IJCAI2025](https://cv-ac.github.io/MiGA2025).

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">PRML 2025</div><img src='images/VQA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

Performance Analysis of Traditional VQA Models Under Limited Computational Resources

[[IEEE PRML'25](https://www.prml.org/prml2025.html)] [[Paper](https://ieeexplore.ieee.org/abstract/document/11159683)]

**Jihao Gu**

</div>
</div>

# ğŸ”¬ Projects

- *2025.09 - 2025.11*: MABench: Towards Fine-grained Micro-Action Understanding

  **Jihao Gu**, [Kun Li](https://kunli-cs.github.io/)

  > - Proposed MA-Bench, a comprehensive benchmark comprising 1,000 videos and a three-tier evaluation architecture that progressively examines micro-action perception, relational comprehension, and interpretive reasoning
  > - Constructed MA-Bench-Train, a large-scale training corpus with 20K videos annotated for detailed motion patterns and finetuned Qwen3-VL-8B on MA-Bench-Train, achieving consistent gains across micro-action reasoning tasks

- *2025.03 - 2025.09*: Text-guided Fine-Grained Video Anomaly

  [Postgraduate Project] [Thesis]

  **Jihao Gu**, [Kaan AkÅŸit (Supervisor)](https://kaanaksit.com/), [He Wang (Co-supervisor)](https://drhewang.com/)

  > - Proposed Text-guided Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large Vision-Language Model (LVLM)
  > - Introduced an Anomaly Heatmap Decoder (AHD) that performs pixel-wise visual-textual feature alignment to generate fine-grained anomaly heatmaps
  > - Designed a Region-aware Anomaly Encoder (RAE) that transforms the heatmaps into learnable textual embeddings, guiding the LVLM to accurately identify and localize anomalous events in videos
  > - Achieved SOTA performance by demonstrating 94.8% micro-AUC and 67.8% / 76.7% accuracy in anomaly heatmaps (RBDC / TBDC) on the UBnormal dataset


- *2024.01 - 2024.06*: Enhancing the Baseline Performance of OrienterNet for Visual Localization

  **Jihao Gu**, [Yan Da (Supervisor)](https://homes.luddy.indiana.edu/yanda/home.html)

  > Our task aims to further optimize OrienterNet, a neural network-based visual localization method designed to achieve accurate localization using 2D public maps (e.g., planar maps). The original approach matches camera-captured images with public maps, effectively addressing localization challenges in GPS-denied scenarios, especially in indoor and complex urban environments.

- *2022.12 - 2023.10*: End-to-End Sign Language Recognition using Transformers, 12/2022 - 10/2023

  **Jihao Gu**, [Shengeng Tang](https://tangshengeng.github.io/)

  > - Read and analyzed two papers in detail, and learned the application of the Transformer architecture in sign language recognition and the design idea of the dual-block module
  > - Used the PyTorch toolkit to reproduce the model in the paper, and achieved similar BLEU and WER scores on the validation set and test set
  > - Combined the dual-block module with the reproduced model, and enhanced the BLEU and WER scores on the validation set and test set by parameter adjustment and multiple rounds of training


- *2021.06 - 2024.06*: Navigation System for Visually Impaired People Based on Visual Ambient Intelligence

  [Undergraduate Project] [[Thesis](https://github.com/momiji-bit/Bachelor-s-Degree-Project/blob/main/[ç»ˆç‰ˆ]è°·çºªè±ª_æ¯•ä¸šè®ºæ–‡.pdf)] [[Project](https://github.com/momiji-bit/UG-Thesis-VEPNav)]

  **Jihao Gu**, [Guo Dan (Supervisor)](https://faculty.hfut.edu.cn/gd/zh_CN/index.htm), [Meng Wang (Co-supervisor)](https://sites.google.com/view/meng-wang/home)

  > - Designed and implemented a navigation system based on visual environmental perception, with accuracy increased by approximately 15% compared with traditional navigation systems
  > - Proposed a deep intelligent interaction-based outdoor assistance method tailored for visually impaired individuals
  > - Developed an offset warning system leveraging semantic segmentation techniques
  > - Introduced a collision warning method utilizing image-based object detection and visual depth estimation
  > - Formulated a route planning method founded on weighted undirected graph principles
  > - Constructed a road image dataset specifically for the Hefei University of Technology campus, which contains over 12,000 images


# ğŸ§¾ Patents

- Blind Travel Obstacle Avoidance Assistance System V1.0 [2023SR0517944]
- Outdoor Visual Impairment Assisting Method based on Deep Intelligent Interaction [[CN114724053A]](https://patents.google.com/patent/CN114724053A/en)  
- Semantic Segmentation-Based Preferential Direction Deviation Early Warning System [[CN114723946A]](https://patents.google.com/patent/CN114723946A/en)  
- Route Planning Method for Visually Impaired People [[CN116448130A]](https://patents.google.com/patent/CN116448130A/en)  
- Collision Warning Method based on Image Target Detection and Depth Estimation [[CN116403146A]](https://patents.google.com/patent/CN116403146A/en)

# ğŸ– Honors and Awards

- *2024.06*: Outstanding Graduate of Hefei University of Technology. 
- *2024.06*: Outstanding Graduation Thesis (Design)
- *2023.09*: First-Class Scholarship
- *2023.09*: "Three Good Students" Award
- *2022.09*: Second-Class Scholarship
- *2022.09*: "Three Good Students" Award
- *2021.09*: Second-Class Scholarship

# ğŸ’¼ Internships

- *2025.09 - now*: Research Assistant, [Computational Light Laboratory](https://complightlab.com), University College London, London, UK
- *2023.07 - 2023.08*: Embedded Development Assistant, Shenzhen Boshengteng Technology Co., Ltd., Shenzhen, China
- *2023.03 - 2023.06*: Algorithm Development Assistant, Shenzhen Boshengteng Technology Co., Ltd., Shenzhen, China
- *2023.01 - 2023.02*: Python Development Engineer Assistant, Beijing Tuosida Technology Development Co., Ltd., Beijing, China

# ğŸ¤ Services

- *2025.12*: Data Chair for The 3rd Micro-Action Analysis Grand Challenge (**ACM MM'26**)
- *2025.11*: Reviewer for IEEE Transactions on Multimedia (**TMM**)
- *2025.11*: Reviewer for ACM Transactions on Multimedia Computing Communications, and Applications (**TOMM**)
- *2025.08*: Volunteer for International Joint Conference on Artificial Intelligence (**IJCAI'25**, Guangzhou)
- *2025.08*: Reviewer for IEEE Transactions on Pattern Analysis and Machine Intelligence (**TPAMI**)
- *2025.04*: Reviewer for Engineering Applications of Artificial Intelligence (**EAAI**Ã—4)
- *2025.04*: Reviewer for Intelligent Data Analysis (**IDA**Ã—2)
- *2025.03*: Reviewer for the 2025 International Joint Conference on Neural Networks (**IJCNNâ€™25**)
- *2024.02*: Reviewer for the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (**SIGIR'24**)

# ğŸ“– Educations

- *09/2024 - 09/2025*: Postgraduate, Computer Graphics, Vision and Imaging , University College London, UK 

  > **Modules:** Machine Vision, Image Processing, Computer Graphics, Machine Learning for Visual Computing 
  >
  > **Optional:** Inverse Problems in lmaging, Acquisition and Processing of 3D Geometry, Numerical Optimisation, Virtual Environments 
  >
  > **Score:** Pass with Distinction (78.30/100)

- *09/2020 - 06/2024*: Undergraduate, Computer Science and Technology, Hefei University of Technology, China 

  > **GPA:** 90.10% (3.85/4.0), Rank: 5/152
